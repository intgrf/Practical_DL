{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "bR2tIHtVlKMp"
   },
   "source": [
    "# Homework 2.2: The Quest For A Better Network\n",
    "\n",
    "In this assignment you will build a monster network to solve Tiny ImageNet image classification.\n",
    "\n",
    "This notebook is intended as a sequel to seminar 3, please give it a try if you haven't done so yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g13vMGTRlKMs"
   },
   "source": [
    "(please read it at least diagonally)\n",
    "\n",
    "* The ultimate quest is to create a network that has as high __accuracy__ as you can push it.\n",
    "* There is a __mini-report__ at the end that you will have to fill in. We recommend reading it first and filling it while you iterate.\n",
    " \n",
    "## Grading\n",
    "* starting at zero points\n",
    "* +20% for describing your iteration path in a report below.\n",
    "* +20% for building a network that gets above 20% accuracy\n",
    "* +10% for beating each of these milestones on __TEST__ dataset:\n",
    "    * 25% (50% points)\n",
    "    * 30% (60% points)\n",
    "    * 32.5% (70% points)\n",
    "    * 35% (80% points)\n",
    "    * 37.5% (90% points)\n",
    "    * 40% (full points)\n",
    "    \n",
    "## Restrictions\n",
    "* Please do NOT use pre-trained networks for this assignment until you reach 40%.\n",
    " * In other words, base milestones must be beaten without pre-trained nets (and such net must be present in the anytask atttachments). After that, you can use whatever you want.\n",
    "* you __can't__ do anything with validation data apart from running the evaluation procedure. Please, split train images on train and validation parts\n",
    "\n",
    "## Tips on what can be done:\n",
    "\n",
    "\n",
    " * __Network size__\n",
    "   * MOAR neurons, \n",
    "   * MOAR layers, ([torch.nn docs](http://pytorch.org/docs/master/nn.html))\n",
    "\n",
    "   * Nonlinearities in the hidden layers\n",
    "     * tanh, relu, leaky relu, etc\n",
    "   * Larger networks may take more epochs to train, so don't discard your net just because it could didn't beat the baseline in 5 epochs.\n",
    "\n",
    "   * Ph'nglui mglw'nafh Cthulhu R'lyeh wgah'nagl fhtagn!\n",
    "\n",
    "\n",
    "### The main rule of prototyping: one change at a time\n",
    "   * By now you probably have several ideas on what to change. By all means, try them out! But there's a catch: __never test several new things at once__.\n",
    "\n",
    "\n",
    "### Optimization\n",
    "   * Training for 100 epochs regardless of anything is probably a bad idea.\n",
    "   * Some networks converge over 5 epochs, others - over 500.\n",
    "   * Way to go: stop when validation score is 10 iterations past maximum\n",
    "   * You should certainly use adaptive optimizers\n",
    "     * rmsprop, nesterov_momentum, adam, adagrad and so on.\n",
    "     * Converge faster and sometimes reach better optima\n",
    "     * It might make sense to tweak learning rate/momentum, other learning parameters, batch size and number of epochs\n",
    "   * __BatchNormalization__ (nn.BatchNorm2d) for the win!\n",
    "     * Sometimes more batch normalization is better.\n",
    "   * __Regularize__ to prevent overfitting\n",
    "     * Add some L2 weight norm to the loss function, PyTorch will do the rest\n",
    "       * Can be done manually or like [this](https://discuss.pytorch.org/t/simple-l2-regularization/139/2).\n",
    "     * Dropout (`nn.Dropout`) - to prevent overfitting\n",
    "       * Don't overdo it. Check if it actually makes your network better\n",
    "   \n",
    "### Convolution architectures\n",
    "   * This task __can__ be solved by a sequence of convolutions and poolings with batch_norm and ReLU seasoning, but you shouldn't necessarily stop there.\n",
    "   * [Inception family](https://hacktilldawn.com/2016/09/25/inception-modules-explained-and-implemented/), [ResNet family](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035?gi=9018057983ca), [Densely-connected convolutions (exotic)](https://arxiv.org/abs/1608.06993), [Capsule networks (exotic)](https://arxiv.org/abs/1710.09829)\n",
    "   * Please do try a few simple architectures before you go for resnet-152.\n",
    "   * Warning! Training convolutional networks can take long without GPU. That's okay.\n",
    "     * If you are CPU-only, we still recomment that you try a simple convolutional architecture\n",
    "     * a perfect option is if you can set it up to run at nighttime and check it up at the morning.\n",
    "     * Make reasonable layer size estimates. A 128-neuron first convolution is likely an overkill.\n",
    "     * __To reduce computation__ time by a factor in exchange for some accuracy drop, try using __stride__ parameter. A stride=2 convolution should take roughly 1/4 of the default (stride=1) one.\n",
    " \n",
    "   \n",
    "### Data augmemntation\n",
    "   * getting 5x as large dataset for free is a great \n",
    "     * Zoom-in+slice = move\n",
    "     * Rotate+zoom(to remove black stripes)\n",
    "     * Add Noize (gaussian or bernoulli)\n",
    "   * Simple way to do that (if you have PIL/Image): \n",
    "     * ```from scipy.misc import imrotate,imresize```\n",
    "     * and a few slicing\n",
    "     * Other cool libraries: cv2, skimake, PIL/Pillow\n",
    "   * A more advanced way is to use torchvision transforms:\n",
    "    ```\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    trainset = torchvision.datasets.ImageFolder(root=path_to_tiny_imagenet, train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "    ```\n",
    "   * Or use this tool from Keras (requires theano/tensorflow): [tutorial](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), [docs](https://keras.io/preprocessing/image/)\n",
    "   * Stay realistic. There's usually no point in flipping dogs upside down as that is not the way you usually see them.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FLDfGOuBlKMu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "DmhTRkiDSWPJ",
    "outputId": "24d6e7df-84bc-47e4-aded-c7617a762366"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8IOOj6x7lKM-",
    "outputId": "d07a318a-3bf0-44d5-a47b-217f059a4eae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./drive/My Drive/data/tiny-imagenet-200.zip\n"
     ]
    }
   ],
   "source": [
    "from tiny_img import download_tinyImg200\n",
    "data_path = './drive/My Drive/data/'\n",
    "download_tinyImg200(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-n5RF__0G3Kh"
   },
   "outputs": [],
   "source": [
    "! rm -r ./tiny-imagenet-200/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TxNhaaxlGhk6"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import glob\n",
    "import os\n",
    "from shutil import move\n",
    "from os.path import join\n",
    "from os import listdir, rmdir\n",
    "\n",
    "target_folder = './tiny-imagenet-200/val/'\n",
    "test_folder   = './tiny-imagenet-200/test/'\n",
    "\n",
    "os.mkdir(test_folder)\n",
    "val_dict = {}\n",
    "with open('./tiny-imagenet-200/val/val_annotations.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        split_line = line.split('\\t')\n",
    "        val_dict[split_line[0]] = split_line[1]\n",
    "        \n",
    "paths = glob.glob('./tiny-imagenet-200/val/images/*')\n",
    "for path in paths:\n",
    "    file = path.split('/')[-1]\n",
    "    folder = val_dict[file]\n",
    "    if not os.path.exists(target_folder + str(folder)):\n",
    "        os.mkdir(target_folder + str(folder))\n",
    "        os.mkdir(target_folder + str(folder) + '/images')\n",
    "    if not os.path.exists(test_folder + str(folder)):\n",
    "        os.mkdir(test_folder + str(folder))\n",
    "        os.mkdir(test_folder + str(folder) + '/images')\n",
    "        \n",
    "        \n",
    "for path in paths:\n",
    "    file = path.split('/')[-1]\n",
    "    folder = val_dict[file]\n",
    "    if len(glob.glob(target_folder + str(folder) + '/images/*')) <25:\n",
    "        dest = target_folder + str(folder) + '/images/' + str(file)\n",
    "    else:\n",
    "        dest = test_folder + str(folder) + '/images/' + str(file)\n",
    "    move(path, dest)\n",
    "\n",
    "rmdir('./tiny-imagenet-200/val/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mlpFO1cFAYHh"
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "   transforms.RandomChoice([\n",
    "   transforms.RandomRotation(25),\n",
    "   transforms.RandomResizedCrop(64, scale=(0.6, 1)),\n",
    "   transforms.RandomHorizontalFlip(p=0.5),\n",
    "   transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15)                       \n",
    "   ]),\n",
    "   transforms.ToTensor()\n",
    "   ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tHYmd5wilKNG"
   },
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder('tiny-imagenet-200/train', transform=transform_train)\n",
    "test_dataset = torchvision.datasets.ImageFolder('tiny-imagenet-200/test', transform=transforms.ToTensor())\n",
    "val_dataset = torchvision.datasets.ImageFolder('tiny-imagenet-200/val', transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M9yGhQcUnEje"
   },
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f7oonHgHlKNc"
   },
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n7Ix6UH1Odqs"
   },
   "outputs": [],
   "source": [
    "num_classes = 200\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, kernel_size=3, padding=3, stride=2),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(64, 128, kernel_size=3, padding=3, stride=2),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(128, 256, kernel_size=3, padding=3, stride=2),\n",
    "    nn.BatchNorm2d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Linear(36864, 4096),\n",
    "    nn.BatchNorm1d(4096),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Linear(4096, 1024),\n",
    "    nn.BatchNorm1d(1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Linear(1024, num_classes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "YsL0QkRcMiU8",
    "outputId": "583af8cb-6e29-4429-8282-b9bf413c52be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 34, 34]           1,792\n",
      "       BatchNorm2d-2           [-1, 64, 34, 34]             128\n",
      "              ReLU-3           [-1, 64, 34, 34]               0\n",
      "            Conv2d-4          [-1, 128, 19, 19]          73,856\n",
      "       BatchNorm2d-5          [-1, 128, 19, 19]             256\n",
      "              ReLU-6          [-1, 128, 19, 19]               0\n",
      "            Conv2d-7          [-1, 256, 12, 12]         295,168\n",
      "       BatchNorm2d-8          [-1, 256, 12, 12]             512\n",
      "              ReLU-9          [-1, 256, 12, 12]               0\n",
      "          Flatten-10                [-1, 36864]               0\n",
      "          Dropout-11                [-1, 36864]               0\n",
      "           Linear-12                 [-1, 4096]     150,999,040\n",
      "      BatchNorm1d-13                 [-1, 4096]           8,192\n",
      "             ReLU-14                 [-1, 4096]               0\n",
      "          Dropout-15                 [-1, 4096]               0\n",
      "           Linear-16                 [-1, 1024]       4,195,328\n",
      "      BatchNorm1d-17                 [-1, 1024]           2,048\n",
      "             ReLU-18                 [-1, 1024]               0\n",
      "          Dropout-19                 [-1, 1024]               0\n",
      "           Linear-20                  [-1, 200]         205,000\n",
      "================================================================\n",
      "Total params: 155,781,320\n",
      "Trainable params: 155,781,320\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 4.32\n",
      "Params size (MB): 594.26\n",
      "Estimated Total Size (MB): 598.62\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model.cuda(), (3, 64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7HCmB072n0C9"
   },
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VgdPTmF6lKOE"
   },
   "outputs": [],
   "source": [
    "# init parameters\n",
    "\n",
    "# learning rate\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# optimizer\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# loss function\n",
    "loss = torch.nn.modules.loss.CrossEntropyLoss()\n",
    "\n",
    "# number of epochs\n",
    "num_epochs = 30\n",
    "\n",
    "# batch size \n",
    "batch_size = 512\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XfMmykn9lKOJ"
   },
   "outputs": [],
   "source": [
    "train_batch_gen = torch.utils.data.DataLoader(train_dataset, \n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7CIsNyJrlKOQ"
   },
   "outputs": [],
   "source": [
    "val_batch_gen = torch.utils.data.DataLoader(val_dataset, \n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vyACyt6HlKOX"
   },
   "outputs": [],
   "source": [
    "def compute_loss(X_batch, y_batch):\n",
    "    X_batch = Variable(torch.FloatTensor(X_batch)).cuda()\n",
    "    y_batch = Variable(torch.LongTensor(y_batch)).cuda()\n",
    "    logits = model.cuda()(X_batch)\n",
    "    return F.cross_entropy(logits, y_batch).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280,
     "referenced_widgets": [
      "06873ccff9ba45078f51ec646c632ad8",
      "a37a819c60294ab8a9feb06bd9ff9fbe",
      "e4c606e516be4817ae19280b511f932c",
      "e696bb7c774847f2917c12d1fc970fac",
      "f95d8d8f51c14932b9a7b2ebde2a9de0",
      "3f3707c7b9a645198065a90300f5faa2",
      "be05ed71493d45388422c6107a5a1a31",
      "506e7e6b6f1c4f71906ecebc28c70ab2"
     ]
    },
    "colab_type": "code",
    "id": "0_p6zfU-lKOi",
    "outputId": "c38a3371-6544-4964-d7a8-7943ae3c354f"
   },
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=7, gamma=0.1)\n",
    "\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "import time\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    model.train(True) # enable dropout / batch_norm training behavior\n",
    "    for i_batch, (X_batch, y_batch) in enumerate(tqdm(train_batch_gen)):\n",
    "        # train on batch\n",
    "        loss = compute_loss(X_batch, y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        train_loss.append(loss.cpu().data.numpy())\n",
    "\n",
    "        logits = model(Variable(torch.FloatTensor(X_batch)).cuda())\n",
    "        y_pred = logits.max(1)[1].data\n",
    "        train_accuracy.append(np.mean( (y_batch.cpu() == y_pred.cpu()).numpy() ))\n",
    "\n",
    "    \n",
    "    model.train(False) # disable dropout / use averages for batch_norm\n",
    "    for X_batch, y_batch in tqdm(val_batch_gen):\n",
    "        logits = model(Variable(torch.FloatTensor(X_batch)).cuda())\n",
    "        y_pred = logits.max(1)[1].data\n",
    "        val_accuracy.append(np.mean( (y_batch.cpu() == y_pred.cpu()).numpy() ))\n",
    "    \n",
    "    scheduler.step()\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(train_loss[-len(train_dataset) // batch_size :])))\n",
    "    print(\"  train accuracy: \\t\\t\\t{:.2f} %\".format(\n",
    "        np.mean(train_accuracy[-len(train_dataset) // batch_size :]) * 100))\n",
    "    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n",
    "        np.mean(val_accuracy[-len(val_dataset) // batch_size :]) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2QupEatAFjZx"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './drive/My Drive/trained_models/seq_final_acc_40.data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zy5nvDzqlKOq"
   },
   "source": [
    "When everything is done, please calculate accuracy on `tiny-imagenet-200/val`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xbp3bgmkteHo"
   },
   "outputs": [],
   "source": [
    "test_batch_gen = torch.utils.data.DataLoader(test_dataset, \n",
    "                                              batch_size=500,\n",
    "                                              shuffle=False,\n",
    "                                              num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fvNhQ_bblKOr"
   },
   "outputs": [],
   "source": [
    "model.train(False) # disable dropout / use averages for batch_norm\n",
    "test_batch_acc = []\n",
    "for (X_batch, y_batch) in test_batch_gen:\n",
    "    logits = model(Variable(torch.FloatTensor(X_batch)).cuda())\n",
    "    y_pred = logits.max(1)[1].data\n",
    "    test_batch_acc.append(np.mean( (y_batch.cpu() == y_pred.cpu()).numpy() ))\n",
    "\n",
    "\n",
    "test_accuracy = np.mean(test_batch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "LxVCE5DHlKOx",
    "outputId": "53a55b14-509b-4249-87ca-823125916262"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t40.18 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pkeUEWdBlKO1"
   },
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "# Report\n",
    "\n",
    "All creative approaches are highly welcome, but at the very least it would be great to mention\n",
    "* the idea;\n",
    "* brief history of tweaks and improvements;\n",
    "* what is the final architecture and why?\n",
    "* what is the training method and, again, why?\n",
    "* Any regularizations and other techniques applied and their effects;\n",
    "\n",
    "\n",
    "There is no need to write strict mathematical proofs (unless you want to).\n",
    " * \"I tried this, this and this, and the second one turned out to be better. And i just didn't like the name of that one\" - OK, but can be better\n",
    " * \"I have analized these and these articles|sources|blog posts, tried that and that to adapt them to my problem and the conclusions are such and such\" - the ideal one\n",
    " * \"I took that code that demo without understanding it, but i'll never confess that and instead i'll make up some pseudoscientific explaination\" - __not_ok__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WccdDC_flKO3"
   },
   "source": [
    "### Hi, my name is `Daria Sharypina`, and here's my story\n",
    "\n",
    "A long time ago in a galaxy far far away, when it was still more than an hour before the deadline, i got an idea:\n",
    "\n",
    "##### I gonna build a neural network, that\n",
    "\n",
    "Я решила начать с `AlexNet5`, попробовала потренировать с разными оптимазерами (`SGD-momentum`, `Adam`, `RMSprop`), пробовала менять learning rate, попробовала разные функции активации, попыталась использовать аугментации. Еще добавляла scheduler для learning rate.\n",
    "\n",
    "How could i be so naive?!\n",
    "\n",
    "##### One day, with no signs of warning,\n",
    "This thing has finally converged and\n",
    "\n",
    "Я реализовала `AlexNet5` через `nn.Sequential`\n",
    "\n",
    "Максимальный результат, который я получила, $\\approx$ 20%\n",
    "\n",
    "Лучше всего получилось с оптимайзерами `Adam`, `RMSprop`, начальным `lr = 1e-3`, `lr_scheduler.StepLR` с `gamma = 0.1` и `step_size = 7`.\n",
    "Эксперименты с функциями активации ничего не дали, особой разницы я не увидела.\n",
    "\n",
    "Потом я добавила `BatchNorm` и получила примерно +5% к accuracy.\n",
    "Аугментации у меня так и не получилось использовать здесь. Каждый раз, когда я добавляла `transform` к тренировочному датасету скор почему-то доходил только до 3% и всё. Я проверяла, что происходит с картинками после аугментаций и они были нормальные, картинки не портились.\n",
    "\n",
    "Потом я решила перейти к `resnet18`, потом к `resnet34` и `resnet50`. Здесь у меня все еще были те же проблемы с аугментациями. С `Adam`, `lr = 1e-3` получилось добиться accuracy около 32%\n",
    "\n",
    "Потом я поняла, что так и не попробовала совсем простые архитектуры и решила вернуться к `nn.Sequential` с парой свёрточных слоёв. У меня получилось добиться 30% с тремя свёрточными слоями, BatchNorm-ом, DropOut-ом и линейным слоем в конце. \n",
    "Аугментации все еще не работали, пока тут что-то мне пришло в голову убрать Normalize из аугментаций. И всё стало хорошо! Аугментации заработали и добавили еще примерно 5%.\n",
    "\n",
    "##### Finally, after 20  iterations, 2 mugs of coffee\n",
    "\n",
    "В результате оставила 3 сверточных слоя, в конце 2 линейных слоя с 2 DropOut-ами + BatchNorm. Использовала `Adam` и `lr = 1e-3`. Добавила scheduler с `step_size=7` и `gamma=0.1`. И использовала аугментации (RandomRotation, RandomResizedCrop, RandomHorizontalFlip, ColorJitter)\n",
    "\n",
    "That, having wasted days of my life training, got\n",
    "\n",
    "* accuracy on training: 50%\n",
    "* accuracy on validation: 38%\n",
    "* accuracy on test: 40%\n",
    "\n",
    "\n",
    "Нужно было сразу начинать с простой архитектуры, постепенно её меняя. А мне же говорили...\n",
    "> Please do try a few simple architectures before you go for resnet-152.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Agff7GpB_m08"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "homework_part2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06873ccff9ba45078f51ec646c632ad8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e4c606e516be4817ae19280b511f932c",
       "IPY_MODEL_e696bb7c774847f2917c12d1fc970fac"
      ],
      "layout": "IPY_MODEL_a37a819c60294ab8a9feb06bd9ff9fbe"
     }
    },
    "3f3707c7b9a645198065a90300f5faa2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "506e7e6b6f1c4f71906ecebc28c70ab2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a37a819c60294ab8a9feb06bd9ff9fbe": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be05ed71493d45388422c6107a5a1a31": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e4c606e516be4817ae19280b511f932c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "  5%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f3707c7b9a645198065a90300f5faa2",
      "max": 196,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f95d8d8f51c14932b9a7b2ebde2a9de0",
      "value": 10
     }
    },
    "e696bb7c774847f2917c12d1fc970fac": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_506e7e6b6f1c4f71906ecebc28c70ab2",
      "placeholder": "​",
      "style": "IPY_MODEL_be05ed71493d45388422c6107a5a1a31",
      "value": " 10/196 [00:06&lt;02:09,  1.44it/s]"
     }
    },
    "f95d8d8f51c14932b9a7b2ebde2a9de0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
